# Python script to download results files from Flywheel
#       uses flywheel-sdk to download data for all sessions in a project
#
#   created March 23, 2021
#   amf
#
#   INPUTS:
#     input_fn              CSV file with subjects and sessions to download
#                               - should include header row ("C_ID","Session")
#                               - each subsequent row should correspond to one subject
#     flywheel_proj_label   name of Flywheel project to download data from
#     include               list of file types to include (optional), e.g., ['dicom'], or ['nifti']
#     exclude               list of file types to exclude (optional)
#
#   OUTPUT:
#     data/ directory with one folder per subject
#
#           REQUIRED DEPENDENCIES:  flywheel-sdk

# ====== user input ====== 
out_dir='results/' # local dir to save the results to
fw_group_label='karthik'
fw_proj_label='Deface'
gear = 'd3b-ped-proc-pipeline' # the gear we're looking for
# include=['']
# exclude=['']


#  ************** MAIN PROCESSES **************
import flywheel
import tarfile
import os
import pandas as pd
from glob import glob
import shutil

if not os.path.isdir(out_dir):
    os.makedirs(out_dir)

# ====== access the flywheel client for your instance ====== 
fw = flywheel.Client() # assumes user is logged in via CLI (otherwise use flywheel.Client(api-key) )

# ====== load subj list CSV file ====== 
project = fw.projects.find_first('label='+fw_proj_label)

for ses in project.sessions.iter():
    ses_label = ses.label
    sub_label = ses.subject.label
    ses = ses.reload()
    analyses = ses.analyses
    if analyses:
        # print(f'{session} has analysis')
        # Check to see if any were generated by our gear
        matches = [asys for asys in analyses if asys.gear_info.get('name') == gear]
       # Loop through the analyses and first make sure we only look at successful runs
        matches = [asys for asys in matches if asys.job.get('state')=='complete']
        print(f'{len(matches)} completed matches')
        # if there are analysis containers that match what we're looking for
        # download each as a tar file
        if len(matches) != 0:
            count=0
            for match in matches:
                match = match.reload()
                out_path = f'{out_dir}/{sub_label}_{ses_label}_{str(count)}.tar'
                count+=1
                fw.download_tar(match, out_path)

# unzip tar files & move output files to sub-dir
zip_files=glob(f'{out_dir}/*.tar')
for file_path in zip_files:
    file = tarfile.open(file_path)
    sub_dir = file_path.split('/')[-1].split('_')[0]
    file.extractall(sub_dir)
    file.close()
    out_files = glob(sub_dir+'*/*/*/output/*')
    for out_fn in out_files:
        shutil.move(out_fn, sub_dir)

# # move report document to sub folder
# for file_path in zip_files:
#     report=file_path.strip('.tar')+'.docx'
#     sub_dir=file_path.strip('.tar')+'/'
#     shutil.move(report,sub_dir)

# remove tar file & analysis directories
for file_path in zip_files:
    os.remove(file_path)
    sub_dir=file_path.strip('.tar')+'/'
    shutil.rmtree(sub_dir+'{gear}*')
